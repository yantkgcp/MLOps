{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f515a8e8-fb34-42b1-bba4-a6b7be7f574e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64955659-5c5f-47f3-b3a4-828582b6c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --user kfp #--user\n",
    "!pip install --user google-cloud-pipeline-components #--user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "505119dc-5230-4157-b8ac-5ff70aab37d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline, component, Artifact, Dataset, Input, Metrics, Model, Output, InputPath, OutputPath\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "import json\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0954e7c-c95f-4ab5-9ebe-0b5870c6299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = '<>project id'\n",
    "BUCKET_NAME=\"gs://\" + PROJECT_ID + \"demo-pipeline-bucket\"\n",
    "BUCKET_NAME=\"gs://\" + '<bucket>' + '/custom_pipeline'\n",
    "REGION=\"us-central1\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718748b-4dc8-4ee4-a96c-0e8ea4c7e8fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset Preparation Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326abfa6-334b-4b34-8963-61326720c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download BigQuery data and convert to CSV\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"pandas\", \"pyarrow\", \"db-dtypes\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"create_dataset.yaml\"\n",
    ")\n",
    "def get_dataframe(\n",
    "    bq_table: str, project: str,\n",
    "    output_data_path: OutputPath(\"Dataset\")\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    #query = \"\"\"select * from \"\"\" + bq_table\n",
    "    bqclient = bigquery.Client(project=project)\n",
    "    #job = bqclient.query(query)\n",
    "    #dataframe= job.to_dataframe()\n",
    "    table = bigquery.TableReference.from_string(bq_table)\n",
    "    rows = bqclient.list_rows(table)\n",
    "    dataframe = rows.to_dataframe(create_bqstorage_client=True,)\n",
    "    dataframe.to_csv(output_data_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ab7d2-e692-4fa4-8db6-0147125e2c74",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training & Evaluation Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49619905-b359-4e54-9a36-13a3a2031804",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train a XGBoost model\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"joblib\", \"imblearn\", \"pyarrow==5.0\", \"google-auth-oauthlib==0.4.1\", \"dill==0.3.1.1\", \"httplib2==0.19\"],\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-8:latest\",\n",
    "    output_component_file=\"train_model_component.yaml\",\n",
    ")\n",
    "def xgb_train(\n",
    "    dataset: Input[Dataset],\n",
    "    metrics: Output[Metrics],\n",
    "    model: Output[Model]\n",
    "):\n",
    "    from joblib import dump\n",
    "    from sklearn.metrics import roc_curve, confusion_matrix, precision_recall_curve, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import xgboost as xgb\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"xgb_train start running...\")\n",
    "    df = pd.read_csv(dataset.path)\n",
    "    df[\"category\"] = df[\"category\"].astype(\"category\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[['card_transactions_amount','card_transactions_transaction_distance','card_transactions_transaction_hour_of_day','category']], \n",
    "        df.is_fraud, test_size=.2, random_state=0)\n",
    "    ros = RandomOverSampler(sampling_strategy='minority', random_state=999)\n",
    "    X_res, y_res = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    X_res[\"category\"] = X_res[\"category\"].astype(\"category\")\n",
    "    \n",
    "    print(\"Define XGBoost Model started...\")\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "    #n_estimators=100,\n",
    "    eval_metric=\"aucpr\", \n",
    "    learning_rate=0.123,\n",
    "    max_depth=10,\n",
    "    enable_categorical=True,\n",
    "    use_label_encoder=False,\n",
    "    tree_method='gpu_hist',\n",
    "    random_state=0)\n",
    "    \n",
    "    print(\"Define XGBoost Model completed...\")\n",
    "    print(\"model fit checkpoint started...\")\n",
    "    \n",
    "    xgb_model.fit(X_res, y_res)\n",
    "    \n",
    "    print(\"model fit checkpoint completed...\")\n",
    "    print(\"metrics calaculation started...\")\n",
    "    \n",
    "    score = xgb_model.score(X_test, y_test)\n",
    "    ros_predicted = xgb_model.predict(X_test)\n",
    "    confusion = confusion_matrix(y_test, ros_predicted)\n",
    "    precision_score = precision_score(y_test, ros_predicted)\n",
    "    recall_score = recall_score(y_test, ros_predicted)\n",
    "    F1 = f1_score(y_test, ros_predicted)\n",
    "    report = classification_report(y_test, ros_predicted, target_names=['not Fraud', 'Fraud'])\n",
    "    print(\"metrics calaculation completed...\")\n",
    "    print(\"metrics logging started...\")\n",
    "    metrics.log_metric(\"framework\", xgb.__version__)\n",
    "    metrics.log_metric(\"accuracy\",score)\n",
    "    #metrics.log_metric(\"confusion matrix\", confusion)\n",
    "    metrics.log_metric(\"precision\", precision_score)\n",
    "    metrics.log_metric(\"recall\", recall_score)\n",
    "    metrics.log_metric(\"F1\", F1)\n",
    "    #metrics.log_metric(\"classification report\", report)\n",
    "    metrics.log_metric(\"SMOTE_dataset_size\", len(df))\n",
    "    print(\"metrics logging completed...\")\n",
    "    print(model.path)\n",
    "    print(type(metrics))\n",
    "    dump(xgb_model, model.path + \".joblib\")\n",
    "    print(\"model uploaded and xgb_train completed...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d4acd-f90c-4260-8773-65eb42c91f3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload Model Component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0357d5d2-6ce7-4f60-a1fa-37d3bc8b4812",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload model to model registry\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"model_upload_component.yaml\",\n",
    ")\n",
    "def upload_model(\n",
    "    model: Input[Model],\n",
    "    project: str,\n",
    "    region: str,\n",
    "    vertex_model: Output[Model]\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    aiplatform.init(project=project, location=region)\n",
    "    \n",
    "    uploaded_model = aiplatform.Model.upload(\n",
    "        display_name=\"xgb-model-pipeline\",\n",
    "        artifact_uri = model.uri.replace(\"model\", \"\"),\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-5:latest\"\n",
    "    )\n",
    "    \n",
    "    vertex_model.uri = uploaded_model.resource_name\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a1f712-d4a5-4f30-8f88-caa973a5f0c0",
   "metadata": {},
   "source": [
    "## Kubeflow Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f21e222e-88e1-4907-91f3-cf25e52cd900",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline\n",
    "\n",
    "@pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline.\n",
    "    name=\"demo-pipeline\",\n",
    ")\n",
    "def pipeline(\n",
    "    bq_table: str = \"\",\n",
    "    output_data_path: str = \"new_train.csv\",\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION\n",
    "):\n",
    "    dataset_task = get_dataframe(bq_table,project)\n",
    "\n",
    "    model_task = xgb_train(\n",
    "        dataset_task.output\n",
    "    ).add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
    "\n",
    "    upload_task = upload_model(\n",
    "        model=model_task.outputs[\"model\"],\n",
    "        project=project,\n",
    "        region=region\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f2615e7-e52a-4036-9a66-e84ac37e96c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1281: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"demo_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47e09f15-6b8c-4d93-a88c-ca2614aa6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "demo = aip.PipelineJob(\n",
    "    display_name=\"demo-pipeline\",\n",
    "    template_path=\"demo_pipeline.json\",\n",
    "    job_id=\"demo-pipeline-{0}\".format(TIMESTAMP),\n",
    "    parameter_values={\"bq_table\": \"<BQ_TABLE_ID>\"},\n",
    "    enable_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1e4eaa1-3bc4-438a-819d-7fe150524fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/16838415269/locations/us-central1/pipelineJobs/demo-pipeline-20220809103627\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/16838415269/locations/us-central1/pipelineJobs/demo-pipeline-20220809103627')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/demo-pipeline-20220809103627?project=16838415269\n"
     ]
    }
   ],
   "source": [
    "service_account='''vertex-managed-notebook@<project_id>.iam.gserviceaccount.com'''\n",
    "demo.submit(service_account=service_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3431a6-a550-4b50-8cfc-38709ae45496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
